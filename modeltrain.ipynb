{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5b6b94-748b-474b-8a1c-24570e5c3fca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Mask R-CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b5ea4-a4c4-446d-8d0f-945d6244c09b",
   "metadata": {},
   "source": [
    "#### 라이브러리 import\n",
    " - **torch, torchvision** : pytorch 및 이미지 처리 위한 라이브러리\n",
    " - **pycocotools.coco** : COCO 형식의 데이터를 처리\n",
    " - **PIL, matplotlib** : 세그멘테이션 결과를 시각\n",
    " - **cv2** : 실시간 컴퓨터 비전을 위한 라이브러리\n",
    " - **sklearn.metrics** : 모델의 성능 평가를 위한 라이브러리\n",
    " - **tqdm** : 학습 진행 척도, 지표를 표시\n",
    " - **os, numpy, random** : 기본적인 데이터 처리를 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ac84a-e026-4bd4-a987-44fbb0bcb1b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558bf75-60c7-4433-9cec-ef41ae686275",
   "metadata": {},
   "source": [
    "#### 난수 고정\n",
    " - **set_seed** 함수 : 난수를 고정하여 결과의 재현성 보장\n",
    "     - `Pytorch` 에서 재현성을 위해 사용하는 난수를 고정\n",
    "         - torch.manual_seed(seed)\n",
    "         - torch.cuda.manual_seed(seed)\n",
    "         - torch.cuda.manual_seed_all(seed)\n",
    "     - `넘파이 및 내장 랜덤` 시드 고정 : 넘파이의 랜덤 시드, 내장 랜덤 시드의 난수 고정\n",
    "         - np.random.seed(seed)\n",
    "         - random.seed(seed)\n",
    "     - `CUDA convolution 연산`의 알고리즘 제어\n",
    "         - deterministic : non-deterministic 알고리즘을 제어하고 deterministic 알고리즘만 허용(True)\n",
    "         - benchmark : CUDA에서 자동으로 알고리즘을 탐색하여 가장 빠른 알고리즘을 적용하는데 이를 해제함(False)\n",
    " - **seed** : 42로 고정(일반적으로 널리 사용되는 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc60dac-3bb7-4b99-a43f-b6fe2589bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f69e2e-4be8-49d6-895a-7d78cdd300e3",
   "metadata": {},
   "source": [
    "#### 데이터셋 클래스 정의 <br>\n",
    " 전체 데이터셋을 모델에 적용하기 위해 전처리, 텐서 적용한 후 최종적으로 img, target 으로 반환<br>\n",
    " **ParkingDataset** 클래스는 초기화하는 `'__init__'`, 이미지와 어노테이션을 전처리하는 `'__getitem__'`, 데이터셋 길이를 반환하는 `'__len__'` 함수로 구성되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773263ae-7f24-42ae-895d-ccf7af6e41e8",
   "metadata": {},
   "source": [
    " - **\\_\\_init__** : 데이터셋 초기화 함수\n",
    "     - **root** : 사용할 이미지 디렉터리\n",
    "     - **COCO(annotation)** : 어노테이션을 COCO 데이터형식으로 처리\n",
    "     - **ids** : 전체 이미지 리스트 \n",
    "     - **max_images** : 최대 사용 이미지 수 지정\n",
    "     - **remove_images** : 제거할 이미지(손상 등) 있으면 번호 지정\n",
    "     - **transforms** : transforms 적용\n",
    " - **\\_\\_getitem__** : 주어진 인덱스에 대한 이미지와 어노테이션을 target으로 전처리\n",
    "     - 이미지를 컬러로 사용(\"RGB\")\n",
    "     - `target` : 아래 항목이 텐서 변환되어 반환\n",
    "         - \"image_id\" \n",
    "         - \"boxes\" : 어노테이션의 'bbox' 좌표를 COCO 형식으로 전환\n",
    "         - \"labels\" : 어노테이션 클래스 번호(1~4)\n",
    "         - \"masks\" : 어노테이션으로 생성된 mask\n",
    "     -  transforms: 증강 또는 추가로 필요한 변형 작업, 입력된 경우 사용   \n",
    " - **\\_\\_len__** : 데이터셋의 전체 길이 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46313063-4315-43b8-bd85-aa2a3aa0008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParkingDataset(Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None, max_images=None, remove_images=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        if max_images is not None:\n",
    "            self.ids = self.ids[:max_images]  # max번째 이미지까지만 사용\n",
    "        if remove_images is not None:\n",
    "            for remove_image in remove_images:\n",
    "                self.ids.remove(remove_image)  # remove_image 제거\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        path = os.path.join(self.root, img_info['file_name'])\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            xmin, ymin, width, height = ann['bbox']\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(ann['category_id'])\n",
    "            masks.append(self.coco.annToMask(ann))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = torch.tensor([img_id])\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe48795-96d2-410a-90c9-b0b747d401d4",
   "metadata": {},
   "source": [
    "####  collate_fn 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48412d5a-4808-418b-9837-7d3fcba380d4",
   "metadata": {},
   "source": [
    " - 데이터셋 길이가 다양하더라도 처리 가능하게 함(모델에 사용하는 데이터셋 길이가 항상 고정된 것은 아니므로 이 함수를 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f0ddd-bb5e-455b-9219-5355cfea0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c3308-65c1-4b6e-b7c4-04819aaaf66a",
   "metadata": {},
   "source": [
    "####  이미지 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cf1d9-4c00-4d4c-bb9c-e3a75eb546d1",
   "metadata": {},
   "source": [
    " - 이미지를 모델에 적용할 수 있게 텐서로 변환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41cc2c-27a6-4298-a862-be01a13178b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86d085-b3ea-4ed7-9083-679947edab47",
   "metadata": {},
   "source": [
    "####  IoU 계산 로직"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924bafa-21f0-4b85-9fef-4ea0c120ffa9",
   "metadata": {},
   "source": [
    " - 예측 결과(pred_boxes)와 실제 타겟(gt_boxes)의 IoU를 계산\n",
    "     - bbox의 intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f867a-27fd-4acb-8e2f-d3ec61c6393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_boxes, gt_boxes):\n",
    "    intersection = np.logical_and(pred_boxes, gt_boxes)\n",
    "    union = np.logical_or(pred_boxes, gt_boxes)\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3172cf-e43e-4608-9ff4-25ae73cb0a84",
   "metadata": {},
   "source": [
    "####  정밀도-재현율 곡선에서 AUC 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf9131-d4e1-4078-bb05-45c860382b64",
   "metadata": {},
   "source": [
    " - model 예측의 성능 평가 지표 AUC 계산(AP : bbox 좌표로 계산\n",
    " - AP(Average Precision)는 Pprecision-Recall 그래프의 선 아래쪽의 면적으로, AP가 높을수록 그 모델의 성능이 높다고 평가할 수 있음\n",
    "     - precisions = tp / (tp + fp) (posivite 예측한 것중 실제 positive 의 비율)\n",
    "     - recalls = tp / (tp + fn) (실제 positive 중 positive 로 예측한 비율)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe6c96-f36e-49c2-a241-28482237c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(pred_boxes, gt_boxes, scores, iou_threshold=0.5):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for threshold in np.linspace(0, 1, num=101):\n",
    "        tp = np.sum((scores >= threshold) & (calculate_iou(pred_boxes, gt_boxes) >= iou_threshold))\n",
    "        fp = np.sum((scores >= threshold) & (calculate_iou(pred_boxes, gt_boxes) < iou_threshold))\n",
    "        fn = np.sum((scores < threshold))\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    \n",
    "    ap = auc(recalls, precisions)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd9bbc-80a4-4fe6-b7d3-202f4ba06d71",
   "metadata": {},
   "source": [
    "####  모델 평가 함수 <br>\n",
    "  검증 데이터셋의 loss 를 계산하여 모델 성능을 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76920dec-21c1-476b-ae1c-627f527bf1c3",
   "metadata": {},
   "source": [
    "**`evaluate_model`**\n",
    " - **model.train()** : 모델을 학습 모드로 설정\n",
    " - **torch.no_grad()** : 검증 단계에서 gradient 연산 과정을 비활성화, 연산 속도 증가\n",
    " - **loss_dict** : 손실 계산을 위해 모델에 이미지와 타겟을 전달\n",
    " - total_loss / len(data_loader_test) : 결과로서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc91d2-5ce1-49bf-bb1c-e4ef16a83d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader_test, device):\n",
    "    model.train()  \n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader_test:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "          \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "\n",
    "    return total_loss / len(data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9df9e-bfd2-4877-aaf0-cf5a5e7e7262",
   "metadata": {},
   "source": [
    "####  모델 학습 함수 (베스트 모델만 저장) <br>\n",
    "  전체 데이터셋을 모델로 학습 및 평가하여 베스트 모델(validation_loss 가 최소인 모델)을 'best_model.pth' 파일로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e19a06-e5f1-4ecc-8c65-30b55f65cb0e",
   "metadata": {},
   "source": [
    "**`train_model`**\n",
    "  - model.train(), os.makedirs : 학습 모드 전환, 결과 저장 디렉터리 생성\n",
    "  - **best_loss** : 첫 epoch 시 저장되도록 초기값을 매우 크게 설정\n",
    "  - **try** : num_epochs 만큼 학습 시작\n",
    "      - **pbar** : 학습 진행 척도를 표시\n",
    "      - **loss_dict** : loss 계산 위해 images, targets을 모델에 넣음\n",
    "      - 역전파 : 학습 중 gradient 값이 계속 저장되어 가는데, 한 epoch 학습 후 다시 초기화하여 진행하여야 함\n",
    "          - 아래 과정을 통해 학습동안 저장된 gradient를 0로 초기화, 역전파와 optimizer 를 다시 진행하도록 세팅 \n",
    "          - **optimizer.zero_grad()** \n",
    "          - **losses.backward()** \n",
    "          - **optimizer.step()**\n",
    "      - **pbar.set_postfix** : 학습 진행 정도와 함께 loss, gpu 사용량 표시\n",
    "      - **evaluate_model** 호출, 검증 데이터셋을 사용하여 성능 확인\n",
    "      - **validation loss** 출력\n",
    "      - 현재 모델이 이전까지의 모델보다 나을 경우 파일명 'best_model.pth'으로 모델 가중치를 저장\n",
    "  - 전체 epoch 학습 후 완료\n",
    "  - **KeyboardInterrupt** : Ctrl+C 입력 시 중단, model state를 파일명 'interrupted_model.pth' 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e22058-3cf0-4c5f-ad71-9db134ce7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, data_loader_test, optimizer, device, num_epochs=25, save_dir='model_weights'):\n",
    "    model.train()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_loss = float('inf')  \n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Training\", ncols=100)\n",
    "            running_loss = 0.0\n",
    "            for i, (images, targets) in pbar:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += losses.item()\n",
    "                pbar.set_postfix(\n",
    "                    {\"Loss\": losses.item(), \"memory_allocated(mb)\": (torch.cuda.memory_allocated() / (1024 ** 2))})\n",
    "\n",
    "\n",
    "            # 검증 데이터셋을 사용하여 성능 평가\n",
    "            validation_loss = evaluate_model(model, data_loader_test, device)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} Validation Loss: {validation_loss}\")\n",
    "\n",
    "\n",
    "            # 현재 모델이 이전까지의 모델보다 나은지 판단하여 저장\n",
    "            if validation_loss < best_loss:\n",
    "                best_loss = validation_loss\n",
    "                save_path = os.path.join(save_dir, 'best_model.pth')\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"New best model saved to {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted. Saving current model weights.\")\n",
    "        save_path = os.path.join(save_dir, 'interrupted_model.pth')\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}. Proceeding to inference.\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10615b88-7ef2-4a82-955c-18a9bd2f0a31",
   "metadata": {},
   "source": [
    "####  이미지에 대한 예측 시각화 <br>\n",
    "  예측하려는 이미지를 모델에 맞게 전처리 후 예측한 결과를 클래스별 지정된 색상의 color mask를 오버레이하여 시각화 함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4383d9-3a67-4524-b07a-eb21550bac0b",
   "metadata": {},
   "source": [
    "**`visualize_predictions`** <br>\n",
    "  - **예측 이미지 전처리 및 모델 예측**\n",
    "      - **model.eval()** : 평가 모드 전환, 드롭아웃 비활성화/학습 중 사용하는 평균, 이동평균 업데이트하지 않음\n",
    "      - **get_transform** : 예측 이미지를 모델에 적용하도록 텐서 변환\n",
    "      - **GPU(device)** 사용 설정\n",
    "      - **torch.no_grad()** : gradient 비활성화\n",
    "      - **prediction** : 모델에 이미지를 넣어서 예측 <br>\n",
    "  - **시각화**\n",
    "      - 이미지를 넘파이 배열 형태로 처리 \n",
    "      - 각 클래스의 색상 지정 (주차 공간: 초록, 도로: 빨강, 사람: 파랑, 탈것: 노랑) \n",
    "      - 예측된 이미지의 클래스(0:배경 제외)별 mask에 색상을 적용\n",
    "      - mask = mask > 0.3 : mask threshold 0.3으로 설정 \n",
    "      - 원본 이미지에 마스크 오버레이된 이미지 출력 및 저장(\"result.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08102198-e533-465e-8cf4-dcaa8ad3e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, img_path, device):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    transform = get_transform(train=False)\n",
    "    img_tensor = transform(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img_tensor])\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # 각 클래스의 색상 지정 (주차 공간, 도로, 탈것, 사람)\n",
    "    colors = {\n",
    "        1: [0, 255, 0],  # 주차 공간: 초록\n",
    "        2: [255, 0, 0],  # 도로: 빨강\n",
    "        3: [0, 0, 255],  # 사람: 파랑\n",
    "        4: [255, 255, 0]  # 탈것: 노랑\n",
    "    }\n",
    "\n",
    "    for i in range(len(prediction[0]['masks'])):\n",
    "        label = prediction[0]['labels'][i].item()\n",
    "        if label in colors:  # 배경(label == 0)은 제외하고 카테고리 시각화\n",
    "            mask = prediction[0]['masks'][i, 0].cpu().numpy()\n",
    "            mask = mask > 0.3  # Threshold 적용\n",
    "            color_mask = np.zeros_like(img_np)\n",
    "            for c in range(3):\n",
    "                color_mask[:, :, c] = np.where(mask, colors[label][c], 0)\n",
    "            img_np = cv2.addWeighted(img_np, 1.0, color_mask, 0.7, 0)  # 원본 이미지에 색상 마스크 오버레이\n",
    "\n",
    "    plt.imshow(img_np)\n",
    "    plt.show()\n",
    "    plt.savefig(\"result.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bde240-0eca-44ed-bc8e-127fa2f3f6f4",
   "metadata": {},
   "source": [
    "####  메인 실행 <br>\n",
    "  데이터셋 전처리, 모델 로드, 학습 및 평가, 예측 후 시각화의 전체 과정을 정의된 함수를 사용하여 실행<br>\n",
    "  데이터 경로, 사전학습 모델 로드, 출력층 커스터마이즈, 모델 가중치, optimizer, 학습 파라미터 등을 학습 환경과 모델 성능에 따라 변경 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274b4fe-26e7-4b63-89c8-f709a22399b1",
   "metadata": {},
   "source": [
    "1. `데이터셋 로드 및 전처리`<br>\n",
    "    - device : GPU 사용 가능 여부 확인(아닌 경우 CPU 사용)\n",
    "    - dataset : 데이터셋 로드(전체 데이터셋 사용)  \n",
    "    - 학습 데이터셋과 테스트 데이터셋 나누기 (8:2 비율)\n",
    "        - 랜덤 스플릿을 적용하여 데이터셋을 나눔\n",
    "        - dataset_train(학습데이터), dataset_test (검증데이터)\n",
    "    - 데이터 로더 생성\n",
    "        - batch size, shuffle(셔플 여부), num_workers(프로세싱 수), collate_fn : 데이터셋에 파라미터를 적용하여 로드하도록 함\n",
    "        - data_loader(학습), data_loader_test(검증)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf97ec-8e9e-466e-a629-92352d22c823",
   "metadata": {},
   "source": [
    "2. `모델 로드 및 커스터마이즈`<br>\n",
    "    - 사전 학습된 Mask R-CNN 모델 로드 및 수정 \n",
    "        - **maskrcnn_resnet50_fpn** 모델 로드 : 사전 학습된 ResNet-FPN backbone의 Mask R-CNN 모델 사용\n",
    "    - 모델의 출력층을 커스터마이즈\n",
    "        - num_classes = 5 (배경 포함 5개의 클래스)\n",
    "        - FastRCNNPredictor : bbox 출력\n",
    "        - MaskRCNNPredictor : 세그멘테이션 mask 출력\n",
    "        - model.to(device) : gpu 사용하여 학습하도록 설정\n",
    "    - 사용자에게 가중치 파일을 직접 지정하여 불러오기\n",
    "        - **load_weights** : 사용자가 지정한 가중치 파일을 사용할지 묻고 없는 경우 scratch 진행\n",
    "    - 옵티마이저 정의 (Adam)\n",
    "        - **optimizer** : Adam 사용(learning rate = 0.001)\n",
    "        - 전 단계에서 학습 후 gradient를 어느 정도 줄일(늘일)지 학습률(learning rate) 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea068d52-cf8b-4b36-a81c-4b6347df20fd",
   "metadata": {},
   "source": [
    "3. `학습 및 예측, 시각화`<br>\n",
    "   - 모델 학습\n",
    "        - train_model 함수 호출 : 위에 정의된 model, data_loader, data_loader_test, optimizer, device 와 에폭 수(num_epochs),\n",
    "          저장 디랙터리(save_dir)를 입력값으로 설정\n",
    "   - 예측 시각화\n",
    "        - img_path : 예측할 이미지 경로\n",
    "        - visualize_predictions 함수 호출 예측 및 시각화\n",
    "   - 최종 모델 저장 (추가 저장)\n",
    "        - 최종적인 모델 가중치를 'mask_rcnn_parking_final.pth' 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9055e-0b01-4b6c-816b-805498e9fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # GPU 사용 가능 여부 확인\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # 데이터셋 로드 (전체 데이터셋 사용)\n",
    "    dataset = ParkingDataset(root='/home/elicer/merged_data/merged_segmentimage',\n",
    "                             annotation='/home/elicer/merged_data/merged_coco.json',\n",
    "                             transforms=get_transform(train=True)\n",
    "                             )\n",
    "\n",
    "    # 학습 데이터셋과 테스트 데이터셋 나누기 (8:2 비율)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    dataset_train, dataset_test = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    data_loader = DataLoader(dataset_train, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "    # 사전 학습된 Mask R-CNN 모델 로드 및 수정\n",
    "    model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # 모델의 출력층을 커스터마이즈 (배경 포함 5개의 클래스)\n",
    "    num_classes = 5  # background + 4 classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        model.roi_heads.mask_predictor.conv5_mask.in_channels, 256, num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # 사용자에게 가중치 파일을 직접 지정하여 불러오기\n",
    "    load_weights = input(\n",
    "        \"Enter the name of the weights file to load (leave blank to start training from scratch): \").strip()\n",
    "\n",
    "    if load_weights:\n",
    "        weight_path = os.path.join('model_weights', load_weights)\n",
    "        if os.path.exists(weight_path):\n",
    "            model.load_state_dict(torch.load(weight_path))\n",
    "            print(f\"Model loaded from {weight_path}\")\n",
    "        else:\n",
    "            print(f\"File {weight_path} does not exist. Starting training from scratch.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch...\")\n",
    "\n",
    "    # 옵티마이저 정의 (Adam으로 변경)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "    # 모델 학습\n",
    "    training_complete = train_model(model, data_loader, data_loader_test, optimizer, device, num_epochs=10,\n",
    "                                    save_dir='model_weights')\n",
    "\n",
    "    # 예측 시각화\n",
    "    img_path = '/home/elicer/parking.jpg'  \n",
    "    visualize_predictions(model, img_path, device)\n",
    "\n",
    "    # 최종 모델 저장 (추가 저장)\n",
    "    final_save_path = os.path.join('model_weights', f'mask_rcnn_parking_final.pth')\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
